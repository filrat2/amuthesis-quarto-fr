---
editor: 
  markdown: 
    wrap: sentence
---

# Metody {#sec-metody}

## Przygotowanie danych {#sec-processing}

### Sentinel-1 {#sec-processing-s1}

Korzystanie z danych radarowych wymaga wcześniejszego przygotowania danych poprzez proces kalibracji, aby zapewnić poprawne wyniki analizy.
Procesy te mogą różnić się w zależności od konkretnego zastosowania, mając na celu dostosowanie danych do specyficznych potrzeb.
W pracy wykorzystano schemat przetwarzania danych Sentinel-1 GRD, który zaproponował @filipponi_2019_s1_workflow, obejmujący:

1.  aktualizację danych o położeniu satelity w momencie zobrazowania poprzez pobranie i zastosowanie pliku orbity;

2.  korekcję szumów termicznych;

3.  korekcję szumów na granicach obrazów;

4.  obliczenie współczynnika rozproszenia wstecznego (ang. *backscatter coefficient*) sigma0 za pomocą kalibracji radiometrycznej;

5.  korekcję topograficzna (ortorektyfikacja za pomocą Copernicus 30 m Global DEM);

6.  konwersję współczynnika rozproszenia wstecznego na dB za pomocą transformacji logarytmicznej

rycina - Sentinel-1 GRD Preprocessing Workflow

Wstępne Przetwarzanie danych dla obu wykorzystywanych polaryzacji (VV i VH) zostało wykonane przy użyciu zestawu narzędzi ESA Sentinel-1 Toolbox (S1TBX) [@s1tbx] w oprogramowaniu SNAP [@snap] przy pomocy narzedzie do przetwarzania grafów (ang. *Graph Processing Tool*, GPT) .
Kolejne etapy przygotowania danych zostały zrealizowane przy wykorzystaniu języka R (@R-base) oraz pakietu *terra* [@R-terra].
Obszar analizy, będący kaflem Sentinel-2 o oznaczeniu 33UWV, znajduje się na granicy dwóch sąsiednich produktów Sentinel-1 GRD. Na potrzeby dalszego przetwarzania, sąsiadujące produkty zostały połączone i odpowiednio ograniczone do obszaru zainteresowania.
Na granicy sąsiednich produktów Sentinel-1 GRD występowała przestrzeń bez danych, co wymagało wypełnienia tego obszaru danymi przy użyciu funkcji *focal* z pakietu *terra*.

rycina - mapa produkty S1 GRD (granice) vs. granice kafla 33UWV Sentinel-2

### Sentinel-2 {#sec-processing-s2}

Przetwarzanie danych Sentinel-2 polegało na sprowadzeniu kanałów o rozdzielczości 20 m do rozdzielczości i siatki kanałów w rozdzielczości 10 m.
Przepróbkowanie (ang. *resampling*) zostało przeprowadzone przy wykorzystaniu języka R (@R-base) oraz funkcji *resample* z pakietu *terra* [@R-terra], wykorzystując interpolację dwuliniową (ang. *bilinear interpolation*).

### Łączenie danych {#sec-processing-data-merging}

Rozdzielczość przestrzenna danych Sentinel-1 GRD, podobnie jak danych Sentinel-2 przegotowanych w sposób przedstawiony w podrozdziale [-@sec-processing-s2] wynosi 10 m. 
Stworzenie spójnych wielokanałowych rastrów wymaga sprowadzenia wszystkich zestawów danych do wspólnej rozdzielczości i siatki.
Dane Sentinel-1 GRD zostały przetransformowane do siatki danych Sentinel-2.
Przepróbkowane do wspólnej rozdzielczości oraz siatki dane zostały następnie użyte do obliczeń filtracji teksturalnych oraz wskaźników teledetekcyjnych.
Po uzyskaniu produktów pochodnych, w zależności od wariantu, dane zostały scalone w kilka wielokanałowych rastrów, które posłużyły do wyodrębnienia zestawu danych treningowych oraz do przeprowadzenia predykcji.

tabela - warianty połączenia danych - np. kanały S2, kanały S2 + indeksy spektralne, kanały S2 + polaryzacje S1 etc.

rycina - schemat przygotowania danych

## Próbki treningowe i testowe {#sec-samples-methods}

Opis wektoryzacji farm fotowoltaicznych na podstawie ortofotomapy i mozaik satelitarnych.
Opis procesu pozyskiwania próbek treningowych, uwzględniając liczbę próbek pozytywnych i negatywnych, ewentualne dobieranie negatywnych próbek z jakiejś konkretnej kategorii pokrycia terenu terenu - do napisania po ostatecznym generowaniu/losowaniu próbek

## Uczenie maszynowe {#sec-machine-learning}

Klasyfikacja obrazów w teledetekcji polega na grupowaniu komórek w niewielkie zestawy klas, aby komórki w tych samych klasach miały podobne właściwości [@ismail_2009_classification].
Istnieje wiele różnych metod klasyfikacji danych teledetekcyjnych.
Stosunkowo nowymi podejściami wykorzystywanymi w tym kontekście są metody oparte na sztucznej inteligencji, takie jak uczenie maszynowe (ang. *Machine Learning*, ML) lub uczenie głębokie (ang. *Deep Learning*, DL) [@hejmanowska_2020_dane].

Uczenie maszynowe stanowi obszar sztucznej inteligencji, koncentrujący się na opracowywaniu algorytmów i modeli statystycznych zapewniających systemom komputerowym możliwość automatycznego uczenia się z danych i wykonywania określonych zadań bez konieczności bezpośredniego programowania.
W przypadku skomplikowanych i złożonych zestawów danych nie jesteśmy w stanie odpowiednio ich zinterpretować oraz wydobyć poprawnych informacji po wizualnym przejrzeniu danych [@mahesh_2019_ml].
Uczenie maszynowe jest wykorzystywane do uczenia maszyn efektywnego przetwarzania danych [@sindayigaya_2022_ml].
Algorytmy uczenia maszynowego można podzielić na cztery główne podejścia: uczenie nienadzorowane (ang. *unsupervised learning*), uczenie nadzorowane (ang. *supervised learning*), uczenie częściowo nadzorowane (ang. *semi-supervised learning*) oraz uczenie przez wzmacnianie (uczenie posiłkowane, ang. *reinforcement learning*) [@sarker_2021_ml].

W ciągu ostaniach dwudziestu lat zaproponowano kilka różnych algorytmów uczenia maszynowego do klasyfikacji obrazów satelitarnych [@sheykhmousa_2020_svm_vs_rf], zazwyczaj wykorzystujące techniki klasyfikacji bez nadzoru i klasyfikacji nadzorowanej [@ismail_2009_classification].

Uczenie nienadzorowane analizuje nieoznakowane zbiory danych bez konieczności ingerencji człowieka.
Uczenie bez nadzoru jest powszechnie stosowane do eksploracji danych, ekstrakcji cech generatywnych, identyfikacji istotnych trendów i struktur oraz grupowania wyników.
Ta technika uczenia maszynowego jest najczęściej używana do grupowania (klastowania), redukcji wielowymiarowości (redukcji cech) oraz identyfikacji skojarzeń i relacji [@sarker_2021_ml].

Nadzorowane algorytmy uczenia maszynowego wykorzystują oznaczone dane treningowe do znajdywania powiązań pomiędzy różnymi zmiennymi.
Proces uczenia nadzorowanego zachodzi, gdy określone cele mają zostać osiągnięte na podstawie konkretnego zestawu danych wejściowych (treningowych).
Dwa główne typy uczenia nadzorowanego to klasyfikacja, która separuje dane, oraz regresja, która dopasowuje dane [@sarker_2021_ml].

W badaniu do klasyfikacji wykorzystano nadzorowaną metodę lasów losowych (ang. *Random Forest*, RF) [@breiman_2001_rf].

### Metoda lasów losowych {#sec-random-forest}

Random Forest stał się jednym z najpopularniejszych klasyfikatorów uczenia maszynowego wykorzystywanych przez społeczność teledetekcyjną ze względu na dokładność jego klasyfikacji oraz wysoką wydajność [@belgiu_2016_rf; @sheykhmousa_2020_svm_vs_rf].
Metoda lasów losowych charakteryzuje się odpornością na szumy (ang. *noise*) i przeuczenie (ang. *overfitting*), ponieważ nie bazuje na ważeniu [@gislason_2006_rf].

Algorytm Random Forest, będący rozwinięciem koncepcji drzew decyzyjnych, operuje na zasadzie ensemble learning, czyli łączenia wielu słabszych modeli (indywidualnych drzew decyzyjnych) w jeden silniejszy model [@aaron_2018_ml; @sekulic_2020_rf_interpolation].
Procedura generuje liczne drzewa decyzyjne, opierając się na losowo wybranym zestawie danych ze zbioru danych uczących oraz losowo wyselekcjonowanych zmiennych klasyfikacyjnych [@breiman_2001_rf].
Pojedyncze drzewo korzysta ze zredukowanej liczby danych treningowych i zmiennych, co sprawia, że drzewa różnią się od siebie i są mniej precyzyjne, ale jednocześnie są też mniej skorelowane, przez co model złożony z wielu drzew będzie bardziej niezawodny [@sekulic_2020_rf_interpolation].
W fazie predykcji każde z drzew w lesie dokonuje prognozy, a ostateczna decyzja jest formułowana na podstawie głosowania większościowego. 
W przypadku klasyfikacji, klasa wybierana jest na podstawie największej liczby głosów. [@malinowski_2020_s2lulc].

## Oprogramowanie

### QGIS

QGIS, dawniej Quantum GIS [@qgis], to wieloplatformowe i wolne oprogramowanie o otwartym kodzie źródłowym przeznaczone do przetwarzania danych przestrzennych, rozwijane przez QGIS Development Team od 2002 roku [@hejmanowska_2020_dane; @flenniken_2020_qgis].
Algorytmy przetwarzania danych przestrzennych zebrane w oprogramowaniu QGIS umożliwiają manipulację danymi rastrowymi oraz wektorowymi, a także prowadzenie analiz i wizualizację wyników [@hejmanowska_2020_dane].
Oprogramowanie QGIS oferuje również możliwość korzystania z wielu zewnętrznych programów, tzw.
wtyczek (ang. *plug-in*) rozszerzających jego funkcjonalność [@hejmanowska_2020_dane].
W repozytorium wtyczek znaleźć można narzędzia do zarządzania danymi, przetwarzania obrazów, wizualizacji, czy wykonania dodatkowych zadań, takich jak np.
nadawanie georeferencji czy klasyfikacja zobrazowań satelitarnych [@hejmanowska_2020_dane].

Oprogramowanie QGIS zostało zastosowane do stworzenia zestawu danych referencyjnych poprzez wizualną interpretację ortofotomapy oraz mozaik satelitarnych.
QGIS dostarcza zaawansowane narzędzia do digitalizacji, umożliwiające rysowanie i edytowanie obiektów wektorowych oraz pozwala na przeglądanie danych przestrzennych dostępnych w Internecie za pomocą usług sieciowych, takich jak WMS, WMTS czy XYZ Tiles.

### SNAP i Sentinel-1 Toolbox

SNAP [@snap], czyli Sentinel Application Platform to platforma oprogramowania rozwijana wspólnie przez firmy Brockmann Consult, SkyWatch i C-S na zlecenie Europejskiej Agencji Kosmicznej (ESA), przeznaczona do naukowego wykorzystania misji optycznych i mikrofalowych Sentinel [@snap-desktop; @esa_snap].
Oprogramowanie SNAP zawiera zestawy narzędzi do wizualizacji, przetwarzania oraz analizy danych teledetekcyjnych, a zaimplementowane narzędzie do przetwarzania grafów (ang. *Graph Processing Tool*, GPT) daje możliwość tworzenia łańcuchów procesów przetwarzania danych zdefiniowanych przez użytkownika [@hejmanowska_2020_dane; @moskolai_2022_s1_workflow].
Struktura przetwarzania grafów (ang. *Graph Processing Framework*, GPF) w oprogramowaniu SNAP służy do wsadowego przetwarzania danych za pośrednictwem języka Extensible Markup Language (XML) [@moskolai_2022_s1_workflow].

Przetwarzanie danych pochodzących z misji Sentinel-1 umożliwia zestaw narzędzi S1TBX [@s1tbx], przeznaczony do przetwarzania danych radarowych.
Zestaw narzędzi Sentinel-1 Toolbox (S1TBX) zawiera narzędzia do kalibracji, filtrowania plamek (tzw. efektu pieprzu i soli), koregistracji, ortorektyfikacji, mozaikowania, konwersji danych, polarymetrii i interferometrii [@sentinel-1-toolbox].
Sentinel-1 Toolbox jest opracowywany dla ESA przez firmę Array we współpracy z DLR, Brockmann Consult i OceanDataLab [@sentinel-1-toolbox].

### Środowisko języka R

Czynności związane z końcowym przygotowaniem danych wejściowych oraz bezpośrednio z uczeniem maszynowym zostały wykonane z wykorzystaniem środowiska języka R [@R-base].
R to wieloplatformowy język programowania o otwartym kodzie źródłowym do obliczeń statystycznych i wizualizacji danych [@lovelace_2019_geocomputation].
Dzięki dużej liczbie pakietów R obsługuje również statystki geoprzestrzenne, modelowanie oraz wizualizację danych przestrzennych [@lovelace_2019_geocomputation].
W pracy wykorzystane zostało zintegrowane środowisko programistyczne (ang. *Integrated Development Environment*, IDE) RStudio [@rstudio_team_2020_rstudio] przeznaczone dla języka R.
Poza standardowymi możliwościami środowiska R, w procesie pracy wykorzystane zostały pakiety stworzone przez społeczność R w celu rozszerzenia funkcjonalności tego języka.
Do operacji na danych rastrowych zastosowano pakiet *terra* [@R-terra], natomiast do przetwarzania danych wektorowych używany był pakiet *sf* [@R-sf].
Obliczanie tekstury obrazu Sum Average wyprowadzonej z macierzy współwystępowania poziomu szarości (ang. *gray-level co-occurrence matrix*, GLCM) zostało wykonane przy pomocy pakietu *GLCMTextures* [@R-GLCMTextures].
Losowe generowanie danych przestrzennych umożliwia pakiet *spatstat.random* [@R-spatstat.random] z rodziny pakietów *spatstat* [@R-spatstat].
Do przeprowadzenia analizy oraz predykcji opartej o elementy uczenia maszynowego wykorzystano pakiet *mlr3* [@R-mlr3], w ramach którego użyty został algorytm lasów losowych zaimplementowany w pakiecie *ranger* [@R-ranger].
Do obliczeń związanych z teksturami obrazu oraz uczeniem maszynowym wykorzystano pakiet *future* [@R-future], umożliwiający równoległe (wielowątkowe) przetwarzanie wyrażeń R, skracające czas realizacji zadań w stosunku do przetwarzania sekwencyjnego.
W funkcji pozwalającej na równoległe obliczanie tektur obrazu wykorzystano operator *%\>%* z pakietu *dplyr* [@R-dplyr], który daje możliwość przekazywania wyniku jednej operacji do następnej.

```{r}
#| label: pakietbib
#| echo: false
#| warning: false
pakiety = c("base", "terra", "sf", "GLCMTextures", "spatstat.random", "spatstat", "mlr3", "ranger", "future", "dplyr")
knitr::write_bib(pakiety, "packages.bib", width = 60)
```
